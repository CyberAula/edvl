{
  "paragraphs": [
    {
      "text": "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\nimport org.apache.spark.ml.regression._\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}",
      "user": "anonymous",
      "dateUpdated": "2021-01-14T15:52:17+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\nimport org.apache.spark.ml.regression._\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1610597013345_1123374352",
      "id": "paragraph_1610596347091_318684688",
      "dateCreated": "2021-01-14T04:03:33+0000",
      "dateStarted": "2021-01-14T15:52:17+0000",
      "dateFinished": "2021-01-14T15:52:19+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:41534"
    },
    {
      "text": "\n  def train(): RandomForestRegressionModel = {\n    val schema = StructType(\n      Array(StructField(\"id\", IntegerType),\n            StructField(\"date\", StringType),\n            StructField(\"time\", IntegerType),\n            StructField(\"items\", IntegerType),\n            StructField(\"day\", IntegerType),\n            StructField(\"month\", IntegerType),\n            StructField(\"year\", IntegerType),\n            StructField(\"weekDay\", IntegerType)\n      ))\n\n    val spark = SparkSession\n      .builder\n      .appName(\"StructuredNetworkWordCount\")\n      .master(\"spark://spark-master:7077\")\n      .getOrCreate()\n    import spark.implicits._\n\n    spark.sparkContext.setLogLevel(\"WARN\")\n\n    // Load and parse the data file, converting it to a DataFrame.\n    val data = spark.read.format(\"csv\")\n      .schema(schema)\n      .option(\"header\", \"true\")\n      .option(\"delimiter\", \",\")\n      .load(\"./prediction-job/carrefour_data.csv\")\n      .drop(\"id\")\n      .withColumnRenamed(\"items\",\"label\")\n\n    val assembler = new VectorAssembler()\n        .setInputCols(Array(\"year\", \"month\", \"day\", \"weekDay\",\"time\" ))\n        .setOutputCol(\"features\")\n\n    // Automatically identify categorical features, and index them.\n    var transformedDf = assembler.transform(data)\n\n    // Split the data into training and test sets (30% held out for testing).\n    val Array(trainingData, testData) = transformedDf.randomSplit(Array(0.8, 0.2))\n\n    // Train a RandomForest model.\n    val rf = new RandomForestRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"features\")\n      .setMaxDepth(30)\n      .setMaxBins(32)\n      .setNumTrees(100)\n\n    val featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .fit(transformedDf)\n\n    // Chain indexer and forest in a Pipeline.\n    val pipeline = new Pipeline()\n    .setStages(Array(featureIndexer, rf))\n    // Train model. This also runs the indexer.\n    val model = pipeline.fit(trainingData)\n    // Make predictions.\n    val predictions = model.transform(testData)\n\n    // Select (prediction, true label) and compute test error.\n    val evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\")\n    val rmse = evaluator.evaluate(predictions)\n    println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse)\n\n    val rfModel = model.stages(1).asInstanceOf[RandomForestRegressionModel]\n    rfModel\n  }\n   ",
      "user": "anonymous",
      "dateUpdated": "2021-01-14T16:41:27+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mtrain\u001b[0m: \u001b[1m\u001b[32m()org.apache.spark.ml.regression.RandomForestRegressionModel\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1610597013347_1836455460",
      "id": "paragraph_1610596466121_860017417",
      "dateCreated": "2021-01-14T04:03:33+0000",
      "dateStarted": "2021-01-14T16:41:27+0000",
      "dateFinished": "2021-01-14T16:41:34+0000",
      "status": "FINISHED",
      "$$hashKey": "object:41535"
    },
    {
      "text": " train().write.overwrite().save(\"./prediction-job/model\")",
      "user": "anonymous",
      "dateUpdated": "2021-01-14T16:41:55+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19, 172.26.0.5, executor 0): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 4416556597546473068, local class serialVersionUID = -3328732449542231715\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n  at org.apache.spark.ml.feature.VectorIndexer.fit(VectorIndexer.scala:145)\n  at train(<console>:88)\n  ... 48 elided\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 4416556597546473068, local class serialVersionUID = -3328732449542231715\n  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://dd2a46ae8cab:4040/jobs/job?id=4",
              "$$hashKey": "object:41867"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1610597013347_922361213",
      "id": "paragraph_1610596621582_1686170581",
      "dateCreated": "2021-01-14T04:03:33+0000",
      "dateStarted": "2021-01-14T16:41:55+0000",
      "dateFinished": "2021-01-14T16:42:03+0000",
      "status": "ERROR",
      "$$hashKey": "object:41536"
    },
    {
      "text": "import org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.fiware.cosmos.orion.spark.connector.{ContentType, HTTPMethod, OrionReceiver, OrionSink, OrionSinkObject}\nimport org.apache.spark.ml.feature.{VectorAssembler}\nimport org.apache.spark.ml.regression.{RandomForestRegressionModel}\nimport org.apache.spark.sql.SparkSession\n\n\n/**\n  * Prediction Job\n  * @author @sonsoleslp\n  */\n\ncase class PredictionResponse(socketId: String, predictionId: String, predictionValue: Int, year: Int, month: Int, day: Int, time: Int) {\n  override def toString :String = s\"\"\"{\n  \"socketId\": { \"value\": \"${socketId}\", \"type\": \"String\"},\n  \"predictionId\": { \"value\":\"${predictionId}\", \"type\": \"String\"},\n  \"predictionValue\": { \"value\":${predictionValue}, \"type\": \"Integer\"},\n  \"year\": { \"value\":${year}, \"type\": \"Integer\"},\n  \"month\": { \"value\":${month}, \"type\": \"Integer\"},\n  \"day\": { \"value\":${day}, \"type\": \"Integer\"},\n  \"time\": { \"value\": ${time}, \"type\": \"Integer\"}\n  }\"\"\".trim()\n}\ncase class PredictionRequest(year: Int, month: Int, day: Int, weekDay: Int, time: Int, socketId: String, predictionId: String)",
      "user": "anonymous",
      "dateUpdated": "2021-01-14T04:09:34+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.fiware.cosmos.orion.spark.connector.{ContentType, HTTPMethod, OrionReceiver, OrionSink, OrionSinkObject}\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.RandomForestRegressionModel\nimport org.apache.spark.sql.SparkSession\ndefined class PredictionResponse\ndefined class PredictionRequest\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1610597013347_1042476437",
      "id": "paragraph_1610595773592_174874110",
      "dateCreated": "2021-01-14T04:03:33+0000",
      "dateStarted": "2021-01-14T04:09:34+0000",
      "dateFinished": "2021-01-14T04:10:53+0000",
      "status": "FINISHED",
      "$$hashKey": "object:41537"
    },
    {
      "text": "  final val URL_CB = \"http://orion:1026/v2/entities/ResTicketPrediction1/attrs\"\n  final val CONTENT_TYPE = ContentType.JSON\n  final val METHOD = HTTPMethod.PATCH\n  final val BASE_PATH = \"./prediction-job\"\n\n\n    val spark = SparkSession\n      .builder\n      .appName(\"PredictionJob\")\n      .master(\"local[*]\")\n      .getOrCreate()\n    import spark.implicits._\n    spark.sparkContext.setLogLevel(\"WARN\")\n\n    val ssc = new StreamingContext(spark.sparkContext, Seconds(1))\n    // ssc.checkpoint(\"./output\")\n\n    // Load the numeric vector assembler\n    //    val vectorAssemblerPath = \"%s/models/numeric_vector_assembler.bin\".format(BASE_PATH)\n    //    val vectorAssembler = VectorAssembler.load(vectorAssemblerPath)\n    val vectorAssembler = new VectorAssembler()\n      .setInputCols(Array(\"year\", \"month\", \"day\", \"weekDay\", \"time\"))\n      .setOutputCol(\"features\")\n\n    // Load model\n    // val randomForestModelPath = \"%s/models/spark_random_forest_classifier.flight_delays.5.0.bin\".format(BASE_PATH)\n    val model = RandomForestRegressionModel.load(BASE_PATH+\"/model\")\n\n    // Create Orion Source. Receive notifications on port 9001\n    val eventStream = ssc.receiverStream(new OrionReceiver(9001))\n\n    // Process event stream to get updated entities\n    val processedDataStream = eventStream\n      .flatMap(event => event.entities)\n      .map(ent => {\n        val year = ent.attrs(\"year\").value.toString.toInt\n        val month = ent.attrs(\"month\").value.toString.toInt\n        val day = ent.attrs(\"day\").value.toString.toInt\n        val time = ent.attrs(\"time\").value.toString.toInt\n        val weekDay = ent.attrs(\"weekDay\").value.toString.toInt\n        val socketId = ent.attrs(\"socketId\").value.toString\n        val predictionId = ent.attrs(\"predictionId\").value.toString\n        PredictionRequest(year, month, day, weekDay, time, socketId, predictionId)\n      })\n\n    // Feed each entity into the prediction model\n    val predictionDataStream = processedDataStream\n      .transform(rdd => {\n        val df = rdd.toDF\n        val vectorizedFeatures  = vectorAssembler\n          .setHandleInvalid(\"keep\")\n          .transform(df)\n        val predictions = model\n          .transform(vectorizedFeatures)\n          .select(\"socketId\",\"predictionId\", \"prediction\", \"year\", \"month\", \"day\", \"time\")\n\n        predictions.toJavaRDD\n    })\n      .map(pred=> PredictionResponse(pred.get(0).toString,\n        pred.get(1).toString,\n        pred.get(2).toString.toFloat.round,\n        pred.get(3).toString.toInt,\n        pred.get(4).toString.toInt,\n        pred.get(5).toString.toInt,\n        pred.get(6).toString.toInt)\n    )\n\n    // Convert the output to an OrionSinkObject and send to Context Broker\n    val sinkDataStream = predictionDataStream\n      .map(res => OrionSinkObject(res.toString, URL_CB, CONTENT_TYPE, METHOD))\n\n    // Add Orion Sink\n   // OrionSink.addSink(sinkDataStream)\n    sinkDataStream.print()\n    predictionDataStream.print()\n    ssc.start()\n    ssc.awaitTermination()\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-14T15:48:34+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 172.26.0.5, executor 0): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 4416556597546473068, local class serialVersionUID = -3328732449542231715\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1382)\n  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1423)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.first(RDD.scala:1422)\n  at org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:615)\n  at org.apache.spark.ml.tree.EnsembleModelReadWrite$.loadImpl(treeModels.scala:427)\n  at org.apache.spark.ml.regression.RandomForestRegressionModel$RandomForestRegressionModelReader.load(RandomForestRegressor.scala:274)\n  at org.apache.spark.ml.regression.RandomForestRegressionModel$RandomForestRegressionModelReader.load(RandomForestRegressor.scala:265)\n  at org.apache.spark.ml.util.MLReadable$class.load(ReadWrite.scala:380)\n  at org.apache.spark.ml.regression.RandomForestRegressionModel$.load(RandomForestRegressor.scala:251)\n  ... 45 elided\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 4416556597546473068, local class serialVersionUID = -3328732449542231715\n  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://dd2a46ae8cab:4040/jobs/job?id=1",
              "$$hashKey": "object:41977"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1610597013348_646529794",
      "id": "paragraph_1610595781082_96915701",
      "dateCreated": "2021-01-14T04:03:33+0000",
      "dateStarted": "2021-01-14T15:48:34+0000",
      "dateFinished": "2021-01-14T15:48:46+0000",
      "status": "ERROR",
      "$$hashKey": "object:41538"
    },
    {
      "text": "import org.apache.spark._\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.fiware.cosmos.orion.spark.connector.OrionReceiver\n\nval spark = SparkSession\n      .builder\n      .appName(\"PredictionJob\")\n      .master(\"spark://spark-master:7077\")\n      .getOrCreate()\nimport spark.implicits._\nspark.sparkContext.setLogLevel(\"WARN\")\n\nval ssc = new StreamingContext(spark.sparkContext, Seconds(1))\n// Create Orion Source. Receive notifications on port 9001\nval eventStream = ssc.receiverStream(new OrionReceiver(9001))\n// Process event stream\nval processedDataStream = eventStream\n  .flatMap(event => event.entities)\n  \nprocessedDataStream.print\nssc.start()\nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2021-01-14T17:06:45+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 26, 172.26.0.5, executor 0): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 4416556597546473068, local class serialVersionUID = -3328732449542231715\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:88)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at org.apache.spark.streaming.scheduler.ReceiverTracker.runDummySparkJob(ReceiverTracker.scala:430)\n  at org.apache.spark.streaming.scheduler.ReceiverTracker.launchReceivers(ReceiverTracker.scala:446)\n  at org.apache.spark.streaming.scheduler.ReceiverTracker.start(ReceiverTracker.scala:160)\n  at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:102)\n  at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:588)\n  at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:583)\n  at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:583)\n  at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n  at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:583)\n  at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:575)\n  ... 48 elided\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 4416556597546473068, local class serialVersionUID = -3328732449542231715\n  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:88)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1610597013350_1677915360",
      "id": "paragraph_1610596175929_139168816",
      "dateCreated": "2021-01-14T04:03:33+0000",
      "dateStarted": "2021-01-14T17:04:55+0000",
      "dateFinished": "2021-01-14T17:05:26+0000",
      "status": "ERROR",
      "$$hashKey": "object:41539"
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1610643673886_920742977",
      "id": "paragraph_1610643673886_920742977",
      "dateCreated": "2021-01-14T17:01:13+0000",
      "status": "READY",
      "$$hashKey": "object:41540"
    }
  ],
  "name": "testtt",
  "id": "2FUH45EZH",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/testtt"
}